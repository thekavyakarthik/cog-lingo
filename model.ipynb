{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: \n",
    "  - set up per metric instead of wer metric\n",
    "  - make transformer code more efficient\n",
    "  - get all the data needed\n",
    "  - train model\n",
    "  - build UI (app maybe for experimentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f8d59fd5a14ffeb3c72d18ef54f3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from interface.TimitInterface import TimitInterface # for getting the data file names in necessary format\n",
    "from phoneme import extract_needed_data # for retriveing necessary information from data file location\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2Config, Wav2Vec2ForCTC # needed to run wav2vec2 model\n",
    "from transformers import TrainingArguments, Trainer # needed to train model\n",
    "from datasets import Dataset, load_metric # to convert extracted data into the ideal format needed for model training; load_metric needed to load wer metric\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e9134c5cb5444ab2494a79dcd17b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e844d8ecf3141c79f170ed78d426818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938fee9ce4f4442f86ba58517ff98e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e52bc1df67c4ffb8622c54547b24a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = TimitInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = dataset.data\n",
    "train_data = dataset.train\n",
    "test_data = dataset.test\n",
    "val_data = dataset.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_keys = list(train_data.keys())\n",
    "test_data_keys = list(test_data.keys())\n",
    "val_data_keys = list(val_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from phoneme import extract_needed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./interface/phoneme.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "#tokenizer = Wav2Vec2CTCTokenizer(\"./phoneme.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"asr-for-phoneme-detection-3\"\n",
    "token = \"\" #insert your token from huggingface here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/kavsss/wav2vec2-tune-timit-asr-for-phoneme-2', endpoint='https://huggingface.co', repo_type='model', repo_id='kavsss/wav2vec2-tune-timit-asr-for-phoneme-2')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the API client\n",
    "api = HfApi()\n",
    "\n",
    "# Create the repository\n",
    "api.create_repo(\n",
    "    repo_id=repo_name,\n",
    "    token=token,\n",
    "    exist_ok=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kavsss/wav2vec2-tune-timit-asr-for-phoneme-2/commit/776edbddbf815c286eac58a5127ecb9e2e28d924', commit_message='Upload tokenizer', commit_description='', oid='776edbddbf815c286eac58a5127ecb9e2e28d924', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    #audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(batch[\"audio_arr\"], sampling_rate=16000).input_values[0]\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"phoneme\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to check that the data is in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/TEST/DR7/FISB0/SA1.WAV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path = './data/'+train_data_keys[0]\n",
    "print(path+'.WAV')\n",
    "audio_arr, line, phoneme = extract_needed_data(path)\n",
    "batch = {\"audio_arr\": audio_arr, \"line\": line, \"phoneme\": phoneme}\n",
    "batch = prepare_dataset(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/TRAIN/DR3/MKXL0/SX195'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'./data/'+val_data_keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00032773,  0.00038493,  0.00050656, ...,  0.00165166,\n",
       "       -0.00132316, -0.00016492], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She had your dark suit in greasy wash water all year.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sh|iy|hv|ae|dcl|d|y|er|dcl|d|aa|r|kcl|k|s|ux|tcl|ih|n|gcl|g|r|iy|z|iy|w|ao|sh|epi|w|ao|dx|er|q|ao|l|y|ih|axr'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0027629 ,  0.00404618,  0.00520826, ...,  0.01614906,\n",
       "       -0.01227374, -0.00120741], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 30,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 23,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 48,\n",
       " 0,\n",
       " 36,\n",
       " 0,\n",
       " 35,\n",
       " 0,\n",
       " 49,\n",
       " 0,\n",
       " 56,\n",
       " 0,\n",
       " 52,\n",
       " 0,\n",
       " 31,\n",
       " 0,\n",
       " 39,\n",
       " 0,\n",
       " 27,\n",
       " 0,\n",
       " 26,\n",
       " 0,\n",
       " 48,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 60,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 58,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 50,\n",
       " 0,\n",
       " 22,\n",
       " 0,\n",
       " 58,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 23,\n",
       " 0,\n",
       " 47,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 37,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 31,\n",
       " 0,\n",
       " 8]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/TEST/DR3/MBWM0/SX134.WAV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path = './data/TEST/DR3/MBWM0/SX134'\n",
    "print(path+'.WAV')\n",
    "audio_arr, line, phoneme = extract_needed_data(path)\n",
    "batch = {\"audio_arr\": audio_arr, \"line\": line, \"phoneme\": phoneme}\n",
    "batch = prepare_dataset(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d|ix|s|eh|m|bcl|b|axr|ix|n|dcl|jh|ae|n|y|ix|w|axr|ih|er|n|ay|s|epi|m|ah|n|t|s|tcl|t|ax-h|s|pcl|p|eh|n|dcl|ix|n|m|ay|ae|m|iy'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = []\n",
    "labels = []\n",
    "for t_path in train_data_keys:\n",
    "    path = './data/'+t_path \n",
    "    audio_arr, line, phoneme = extract_needed_data(path)\n",
    "    batch = {\"audio_arr\": audio_arr, \"line\": line, \"phoneme\": phoneme}\n",
    "    batch = prepare_dataset(batch)\n",
    "    #timit_train[\"features\"][\"input_values\"].append(batch[\"input_values\"])\n",
    "    #timit_train[\"features\"][\"labels\"].append(batch[\"labels\"])\n",
    "    input_values.append(batch[\"input_values\"])\n",
    "    labels.append(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset from the lists\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_values\": input_values,\n",
    "    \"labels\": labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = []\n",
    "labels = []\n",
    "for t_path in test_data_keys:\n",
    "    path = './data/'+t_path \n",
    "    audio_arr, line, phoneme = extract_needed_data(path)\n",
    "    batch = {\"audio_arr\": audio_arr, \"line\": line, \"phoneme\": phoneme}\n",
    "    batch = prepare_dataset(batch)\n",
    "    #timit_test[\"features\"][\"input_values\"].append(batch[\"input_values\"])\n",
    "    #timit_test[\"features\"][\"labels\"].append(batch[\"labels\"])\n",
    "    input_values.append(batch[\"input_values\"])\n",
    "    labels.append(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_values\": input_values,\n",
    "    \"labels\": labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = []\n",
    "labels = []\n",
    "for t_path in val_data_keys:\n",
    "    path = './data/'+t_path \n",
    "    audio_arr, line, phoneme = extract_needed_data(path)\n",
    "    batch = {\"audio_arr\": audio_arr, \"line\": line, \"phoneme\": phoneme}\n",
    "    batch = prepare_dataset(batch)\n",
    "    #timit_val[\"features\"][\"input_values\"].append(batch[\"input_values\"])\n",
    "    #timit_val[\"features\"][\"labels\"].append(batch[\"labels\"])\n",
    "    input_values.append(batch[\"input_values\"])\n",
    "    labels.append(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset.from_dict({\n",
    "    \"input_values\": input_values,\n",
    "    \"labels\": labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavya\\AppData\\Local\\Temp\\ipykernel_5972\\4113679689.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_metric\n",
    "\n",
    "wer_metric = load_metric(\"wer\", trust_remote_code=True)\n",
    "# a little unsure if wer is working\n",
    "# wer is not working, we need per metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kavsss/wav2vec2-tune-timit-asr-for-phoneme were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at kavsss/wav2vec2-tune-timit-asr-for-phoneme and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import Wav2Vec2Config, Wav2Vec2ForCTC\n",
    "\n",
    "# Load your current model config\n",
    "# config = Wav2Vec2Config.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "config = Wav2Vec2Config.from_pretrained(\"kavsss/asr-for-phoneme-detection-2\")\n",
    "\n",
    "# Update the vocab_size\n",
    "config.vocab_size = 64 # sinze there are 61 phonemes sounds and 3 special tokens on top of that\n",
    "config.ctc_loss_reduction = \"mean\"\n",
    "config.pad_token_id=processor.tokenizer.pad_token_id\n",
    "\n",
    "# Save the updated config and load the model\n",
    "config.save_pretrained(\"./asr-for-phoneme-detection-3\")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\", config=config)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"kavsss/asr-for-phoneme-detection-2\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=4, #og: 32\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,#og: 30\n",
    "  fp16=False,\n",
    "  gradient_checkpointing=True, \n",
    "  save_steps=500,\n",
    "  eval_steps=500,\n",
    "  logging_steps=500,\n",
    "  learning_rate=1e-4,\n",
    "  #no_cuda=True,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f421cf06b7743d1b19893165fa8e08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7602, 'grad_norm': 10.608319282531738, 'learning_rate': 5e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd9d1e017e94169a8a3b5e1a14e87bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8088061213493347, 'eval_wer': 0.3476037284500536, 'eval_runtime': 173.6748, 'eval_samples_per_second': 1.935, 'eval_steps_per_second': 0.242, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5624, 'grad_norm': 2.1539642810821533, 'learning_rate': 0.0001, 'epoch': 1.49}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f06c6b34384e7386f0efd8ad53d1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36854758858680725, 'eval_wer': 0.18444279468778355, 'eval_runtime': 154.9994, 'eval_samples_per_second': 2.168, 'eval_steps_per_second': 0.271, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.395, 'grad_norm': 2.0172410011291504, 'learning_rate': 7.037914691943128e-05, 'epoch': 2.23}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92764c0ead0e4d708ec18e49a0d1f270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2887093126773834, 'eval_wer': 0.15763424894828013, 'eval_runtime': 132.8573, 'eval_samples_per_second': 2.529, 'eval_steps_per_second': 0.316, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3096, 'grad_norm': 2.0259854793548584, 'learning_rate': 4.075829383886256e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e185feee8754ea29c3e5ff308e04c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2662785053253174, 'eval_wer': 0.1502103439742638, 'eval_runtime': 149.6909, 'eval_samples_per_second': 2.245, 'eval_steps_per_second': 0.281, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2508, 'grad_norm': 2.8955981731414795, 'learning_rate': 1.113744075829384e-05, 'epoch': 3.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb236eff44f84790862f15d2a54299df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2475772202014923, 'eval_wer': 0.13808463251670378, 'eval_runtime': 119.8119, 'eval_samples_per_second': 2.804, 'eval_steps_per_second': 0.351, 'epoch': 3.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11788.4144, 'train_samples_per_second': 0.912, 'train_steps_per_second': 0.228, 'train_loss': 0.8129628811563764, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2688, training_loss=0.8129628811563764, metrics={'train_runtime': 11788.4144, 'train_samples_per_second': 0.912, 'train_steps_per_second': 0.228, 'total_flos': 3.037705722038615e+17, 'train_loss': 0.8129628811563764, 'epoch': 4.0})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kavsss/wav2vec2-tune-timit-asr-for-phoneme-2/commit/8eee5ea621930d81c702260c5171f42bce317ee9', commit_message='wav2vec2-tune-timit-asr-for-phoneme-2', commit_description='', oid='8eee5ea621930d81c702260c5171f42bce317ee9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(repo_name)#, commit_message=\"training model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accent-capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
