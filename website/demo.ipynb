{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      " - pytorch\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - noisereduce\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - defaults\n",
      "  - https://conda.anaconda.org/conda-forge/win-64\n",
      "  - https://conda.anaconda.org/pytorch/noarch\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "  - https://conda.anaconda.org/pytorch/win-64\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%conda install noisereduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (processing_utils.py, line 448)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3508\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[1], line 6\u001b[0m\n    import gradio as gr\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\gradio\\__init__.py:3\u001b[0m\n    import gradio._simple_templates\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\gradio\\_simple_templates\\__init__.py:1\u001b[0m\n    from .simpledropdown import SimpleDropdown\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\gradio\\_simple_templates\\simpledropdown.py:7\u001b[0m\n    from gradio.components.base import Component, FormComponent\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\gradio\\components\\__init__.py:1\u001b[0m\n    from gradio.components.annotated_image import AnnotatedImage\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\gradio\\components\\annotated_image.py:14\u001b[1;36m\n\u001b[1;33m    from gradio import processing_utils, utils\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\kavya\\anaconda3\\envs\\accent-capstone\\lib\\site-packages\\gradio\\processing_utils.py:448\u001b[1;36m\u001b[0m\n\u001b[1;33m    sync_client.stream(\"GET\", url, follow_redirects=True) as r,\u001b[0m\n\u001b[1;37m                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variable to handle OpenMP conflict, deleting this will take down prod\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import librosa\n",
    "# import noisereduce as nr\n",
    "\n",
    "\n",
    "# Function definitions for each step (placeholders)\n",
    "def preprocess_audio(audio):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio, sr=None)\n",
    "    # Noise reduction\n",
    "    #reduced_noise = nr.reduce_noise(y=y, sr=sr)\n",
    "    # Normalization\n",
    "#     normalized_audio = librosa.util.normalize(reduced_noise)\n",
    "    normalized_audio = librosa.util.normalize(sr)\n",
    "    return normalized_audio, sr\n",
    "\n",
    "def segment_speech(audio):\n",
    "    # Implement speech segmentation\n",
    "    segmented_speech = audio  # Placeholder for actual segmentation\n",
    "    return segmented_speech\n",
    "\n",
    "def transcribe_speech(audio, sr):\n",
    "    # Implement speech transcription using an ASR system\n",
    "    transcription = \"This is a sample transcription.\"  # Placeholder for actual transcription\n",
    "    return transcription\n",
    "\n",
    "def phoneme_analysis(transcription, standard_text):\n",
    "    # Implement phoneme analysis\n",
    "    phoneme_comparison = \"Phoneme comparison results\"  # Placeholder for actual analysis\n",
    "    return phoneme_comparison\n",
    "\n",
    "def calculate_pronunciation_score(phoneme_comparison):\n",
    "    # Calculate pronunciation score based on phoneme comparison\n",
    "    score = 85  # Placeholder\n",
    "    return score\n",
    "\n",
    "def generate_feedback(score):\n",
    "    # Generate visual feedback and improvement tips based on the score\n",
    "    feedback = \"Your pronunciation score is 85. Try to improve on specific phonemes.\"  # Placeholder for actual feedback\n",
    "    return feedback\n",
    "\n",
    "def mispronunciation_detection(audio, standard_text):\n",
    "    preprocessed_audio, sr = preprocess_audio(audio)\n",
    "    segmented_speech = segment_speech(preprocessed_audio)\n",
    "    transcription = transcribe_speech(segmented_speech, sr)\n",
    "    phoneme_comparison = phoneme_analysis(transcription, standard_text)\n",
    "    score = calculate_pronunciation_score(phoneme_comparison)\n",
    "    feedback = generate_feedback(score)\n",
    "    return transcription, score, feedback\n",
    "\n",
    "# Load the flowchart image\n",
    "flowchart_image_path = \"./public/image.png\"\n",
    "if os.path.exists(flowchart_image_path):\n",
    "    flowchart_image = Image.open(flowchart_image_path)\n",
    "else:\n",
    "    flowchart_image = None\n",
    "\n",
    "# Define the Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h1>Mispronunciation Detection and Correction System</h1>\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        if flowchart_image:\n",
    "            gr.Image(flowchart_image, label=\"Flowchart\")\n",
    "        else:\n",
    "            gr.Markdown(\"Flowchart image not found.\")\n",
    "        \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            audio_input = gr.Audio(type=\"numpy\", label=\"Record Your Speech\")\n",
    "            standard_text_input = gr.Textbox(label=\"Standard Text\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            transcription_output = gr.Textbox(label=\"Transcription\")\n",
    "            score_output = gr.Number(label=\"Pronunciation Score\")\n",
    "            feedback_output = gr.Textbox(label=\"Feedback\")\n",
    "\n",
    "    submit_button.click(mispronunciation_detection, \n",
    "                        inputs=[audio_input, standard_text_input], \n",
    "                        outputs=[transcription_output, score_output, feedback_output])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accent-capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
